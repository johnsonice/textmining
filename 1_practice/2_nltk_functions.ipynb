{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK resources\n",
    "\n",
    "The first time you run anything using NLTK, you'll want to go ahead and download the additional resources that aren't distributed directly with the NLTK package. Upon running the nltk.download() command below, the the NLTK Downloader window will pop-up. In the Collections tab, select \"all\" and click on Download. As mentioned earlier, this may take several minutes depending on your network connection speed, but you'll only ever need to run it a single time.\n",
    "\n",
    "or you can download some of the functions you need to use \n",
    "- nltk.download_shell()\n",
    "- hit d and and type in all to down all packages \n",
    "- install averaged_perceptron_tagger\n",
    "- install maxent_ne_chunker\n",
    "- install words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting text from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://venturebeat.com/2014/07/04/facebooks-little-social-experiment-got-you-bummed-out-get-over-it/\"\n",
    "html = urlopen(url).read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stripping-out HTML formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readability, which pulls the main body content out of an HTML document and subsequently \"cleans it up.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TITLE *** \n",
      "\"Facebook’s little social experiment got you bummed out? Get over it | VentureBeat | Social | by Simon Cohen\"\n",
      "\n",
      "*** CONTENT *** \n",
      "\"\n",
      "OP-ED — You would think by the reaction some are having to it that Facebook’s recent admission that it experimented with some people’s feeds is tantamount to Watergate.\n",
      "You would think there had been[...]\"\n"
     ]
    }
   ],
   "source": [
    "from readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "readable_article = Document(html).summary()\n",
    "readable_title = Document(html).title()\n",
    "soup = BeautifulSoup(readable_article,\"lxml\")\n",
    "body = soup.get_text()\n",
    "\n",
    "print ('*** TITLE *** \\n\\\"' + readable_title + '\\\"\\n')\n",
    "print ('*** CONTENT *** \\n\\\"' + body[:200] + '[...]\\\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOP-ED — You would think by the reaction some are having to it that Facebook’s recent admission that it experimented with some people’s feeds is tantamount to Watergate.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenize by sentence\n",
    "sent_tokenize(body)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OP-ED', '—', 'You', 'would', 'think', 'by', 'the', 'reaction', 'some', 'are']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenize by word\n",
    "word_tokenize(body)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 43),\n",
       " ('the', 38),\n",
       " ('’', 32),\n",
       " ('of', 29),\n",
       " (',', 28),\n",
       " ('to', 22),\n",
       " ('that', 18),\n",
       " ('s', 18),\n",
       " ('Facebook', 17),\n",
       " ('it', 16)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "tokens = [word for sent in sent_tokenize(body) for word in word_tokenize(sent)]\n",
    "\n",
    "# show most common words \n",
    "vocab_count = Counter(tokens)\n",
    "vocab_count.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming and lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming: wolv\n",
      "lemmatizing: wolf\n"
     ]
    }
   ],
   "source": [
    "## this is stemming -- not very good \n",
    "porter_stemmer = PorterStemmer()\n",
    "print('stemming:', porter_stemmer.stem('wolves'))\n",
    "\n",
    "## this is lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('lemmatizing:', lemmatizer.lemmatize('wolves'))\n",
    "\n",
    "## process our data \n",
    "stemmed_tokens = [porter_stemmer.stem(t) for t in tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = stopwords.words('english')\n",
    "stopw[:5]\n",
    "\n",
    "## you can also import your own stopword list from a file \n",
    "## from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "#  stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "## filter stop words\n",
    "lemmatized_tokens_no_stop = [t for t in lemmatized_tokens if t not in stopw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN'), ('learning', 'NN'), ('is', 'VBZ'), ('great', 'JJ')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "## example of pos tagging \n",
    "nltk.pos_tag(\"Machine learning is great\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### simple graph to show sentance structure\n",
    "s = \"Albert Einstein was born on March 14, 1879\"\n",
    "#s = \"Chengyu Huang is a chinese name\"\n",
    "tags = nltk.pos_tag(s.split())\n",
    "result = nltk.ne_chunk(tags)  ## stored the reulst,for some reason, when you tried to print it out, it will give you an error  \n",
    "result.draw()                 ## it will draw a recognized chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking with regx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## group all adj + name together \n",
    "## this is just a basic chunk\n",
    "chunkGram = r\"\"\"chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged[0])\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinking with regx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## remove tings from chunks \n",
    "chinkGram = r\"\"\"chunk: {<.*>+}\n",
    "                        }<VB.?|IN|DT|TO>{\"\"\"   ## remove any VB* or IN or DT from chunk\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged[0])\n",
    "chunked.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
