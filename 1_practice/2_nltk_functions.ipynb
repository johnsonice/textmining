{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download_shell()\n",
    "# hit d and and type in all to down all packages \n",
    "# install averaged_perceptron_tagger\n",
    "# install maxent_ne_chunker\n",
    "# install words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\nThis is an sentence.',\n",
       " \"We want to make sure that people Mr. Huang and some decimal \\ndegits like 0.2, these won't be treated as seperate sentences.\",\n",
       " 'We now just add another sentence in the end.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenize by sentence\n",
    "p = ''' \n",
    "This is an sentence. We want to make sure that people Mr. Huang and some decimal \n",
    "degits like 0.2, these won't be treated as seperate sentences. \n",
    "We now just add another sentence in the end. \n",
    "'''\n",
    "sent_tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'an', 'sentence', '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenize by word\n",
    "word_tokenize(p)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopw = stopwords.words('english')\n",
    "stopw[:5]\n",
    "\n",
    "## you can also import your own stopword list from a file \n",
    "## from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "#  stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming and lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming: wolv\n",
      "lemmatizing: wolf\n"
     ]
    }
   ],
   "source": [
    "## example of stemming and lemmatization \n",
    "## lemmatization is usually smoother than stamming\n",
    "## you can see stamming as a more curde version of lemmatization\n",
    "\n",
    "## this is stemming -- not very good \n",
    "porter_stemmer = PorterStemmer()\n",
    "print('stemming:', porter_stemmer.stem('wolves'))\n",
    "\n",
    "## this is lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print('lemmatizing:', lemmatizer.lemmatize('wolves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'NN'), ('learning', 'NN'), ('is', 'VBZ'), ('great', 'JJ')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "## example of pos tagging \n",
    "nltk.pos_tag(\"Machine learning is great\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04020979020979021 0.07357859531772576 0.03836930455635491 5720 299 230 22\n",
      "0.002972027972027972 0.0033444816053511705 0.002951484965873455 5720 299 17 1\n",
      "0.0008741258741258741 0.006688963210702341 0.0005534034311012728 5720 299 5 2\n",
      "0.0024475524475524478 0.016722408026755852 0.0016602102933038186 5720 299 14 5\n",
      "0.0013986013986013986 0.0033444816053511705 0.0012912746725696365 5720 299 8 1\n",
      "0.0034965034965034965 0.0033444816053511705 0.003504888396974728 5720 299 20 1\n",
      "0.017132867132867134 0.04013377926421405 0.01586423169156982 5720 299 98 12\n",
      "0.005244755244755245 0.006688963210702341 0.005165098690278546 5720 299 30 2\n",
      "0.005944055944055944 0.026755852842809364 0.004796163069544364 5720 299 34 8\n",
      "0.04493006993006993 0.043478260869565216 0.04501014572957019 5720 299 257 13\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.016258741258741258 0.0802675585284281 0.012728278915329275 5720 299 93 24\n",
      "0.001048951048951049 0.0033444816053511705 0.0009223390518354548 5720 299 6 1\n",
      "0.029545454545454545 0.006688963210702341 0.030806124331304186 5720 299 169 2\n",
      "0.0015734265734265735 0.0033444816053511705 0.0014757424829367274 5720 299 9 1\n",
      "0.0038461538461538464 0.006688963210702341 0.003689356207341819 5720 299 22 2\n",
      "0.003146853146853147 0.006688963210702341 0.002951484965873455 5720 299 18 2\n",
      "0.0022727272727272726 0.0033444816053511705 0.002213613724405091 5720 299 13 1\n",
      "0.01381118881118881 0.020066889632107024 0.013466150156797639 5720 299 79 6\n",
      "0.006118881118881119 0.0033444816053511705 0.006271905552481092 5720 299 35 1\n",
      "0.0026223776223776225 0.010033444816053512 0.002213613724405091 5720 299 15 3\n",
      "0.009965034965034964 0.006688963210702341 0.010145729570190002 5720 299 57 2\n",
      "0.0012237762237762239 0.016722408026755852 0.00036893562073418186 5720 299 7 5\n",
      "0.0015734265734265735 0.006688963210702341 0.0012912746725696365 5720 299 9 2\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.0012237762237762239 0.006688963210702341 0.0009223390518354548 5720 299 7 2\n",
      "0.00034965034965034965 0.006688963210702341 0.0 5720 299 2 2\n",
      "0.0005244755244755245 0.006688963210702341 0.00018446781036709093 5720 299 3 2\n",
      "0.004370629370629371 0.0033444816053511705 0.004427227448810182 5720 299 25 1\n",
      "0.003146853146853147 0.013377926421404682 0.002582549345139273 5720 299 18 4\n",
      "0.0006993006993006993 0.0033444816053511705 0.0005534034311012728 5720 299 4 1\n",
      "0.0013986013986013986 0.0033444816053511705 0.0012912746725696365 5720 299 8 1\n",
      "0.0022727272727272726 0.0033444816053511705 0.002213613724405091 5720 299 13 1\n",
      "0.0013986013986013986 0.0033444816053511705 0.0012912746725696365 5720 299 8 1\n",
      "0.0024475524475524478 0.0033444816053511705 0.002398081534772182 5720 299 14 1\n",
      "0.0024475524475524478 0.006688963210702341 0.002213613724405091 5720 299 14 2\n",
      "0.0006993006993006993 0.0033444816053511705 0.0005534034311012728 5720 299 4 1\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.0008741258741258741 0.006688963210702341 0.0005534034311012728 5720 299 5 2\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.013111888111888112 0.010033444816053512 0.013281682346430549 5720 299 75 3\n",
      "0.003146853146853147 0.0033444816053511705 0.003135952776240546 5720 299 18 1\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.001048951048951049 0.013377926421404682 0.00036893562073418186 5720 299 6 4\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.00017482517482517483 0.0033444816053511705 0.0 5720 299 1 1\n",
      "0.004370629370629371 0.0033444816053511705 0.004427227448810182 5720 299 25 1\n",
      "0.015559440559440559 0.0033444816053511705 0.016233167312304002 5720 299 89 1\n",
      "0.0008741258741258741 0.006688963210702341 0.0005534034311012728 5720 299 5 2\n",
      "0.0005244755244755245 0.0033444816053511705 0.00036893562073418186 5720 299 3 1\n",
      "0.00034965034965034965 0.0033444816053511705 0.00018446781036709093 5720 299 2 1\n",
      "0.0019230769230769232 0.0033444816053511705 0.0018446781036709095 5720 299 11 1\n",
      "0.00034965034965034965 0.0033444816053511705 0.00018446781036709093 5720 299 2 1\n",
      "0.0015734265734265735 0.0033444816053511705 0.0014757424829367274 5720 299 9 1\n",
      "0.00034965034965034965 0.0033444816053511705 0.00018446781036709093 5720 299 2 1\n",
      "0.001048951048951049 0.0033444816053511705 0.0009223390518354548 5720 299 6 1\n"
     ]
    }
   ],
   "source": [
    "## another examplem, train the sentence tokenizer by your self \n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text) ## unsurprised training model \n",
    "toeknized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "def process_content(i):\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    return tagged\n",
    "tagged = [process_content(i) for i in toeknized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagging Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### simple graph to show sentance structure\n",
    "s = \"Albert Einstein was born on March 14, 1879\"\n",
    "#s = \"Chengyu Huang is a chinese name\"\n",
    "tags = nltk.pos_tag(s.split())\n",
    "result = nltk.ne_chunk(tags)  ## stored the reulst,for some reason, when you tried to print it out, it will give you an error  \n",
    "result.draw()                 ## it will draw a recognized chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking with regx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## group all adj + name together \n",
    "## this is just a basic chunk\n",
    "chunkGram = r\"\"\"chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged[0])\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinking with regx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## remove tings from chunks \n",
    "chinkGram = r\"\"\"chunk: {<.*>+}\n",
    "                        }<VB.?|IN|DT|TO>{\"\"\"   ## remove any VB* or IN or DT from chunk\n",
    "chunkParser = nltk.RegexpParser(chunkGram)\n",
    "chunked = chunkParser.parse(tagged[0])\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entity recognization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "namedEnt = nltk.ne_chunk(tagged[1])\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
