{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Example in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.most_similar('ok',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## word to vector\n",
    "model['me'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = '../../twit_data/training.csv'\n",
    "data = pd.read_csv(file_path,encoding='latin1',header=None)\n",
    "columns = ['sentiment','text']\n",
    "data=data[[0,5]]\n",
    "data.columns=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chengyu/anaconda3/envs/my_root/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "## keep only positive and negeative values \n",
    "data = data[((data['sentiment']==4) | (data['sentiment']==0))]\n",
    "data['sentiment'][data['sentiment']==4]  = 1 \n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## clean up text and make them into list of tokens \n",
    "def clean_text(text):\n",
    "    return text.lower().replace('\\n','').split()\n",
    "\n",
    "data['text'] = data['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## assign training and testing data \n",
    "x_train = data['text'].tolist()\n",
    "y_train = data['sentiment'].tolist()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### initialize model and build vocabulary \n",
    "n_dim = 300\n",
    "window = 7 \n",
    "downsampling = 0.001\n",
    "seed = 1 \n",
    "num_workers = 8\n",
    "min_count = 3 \n",
    "twit_w2v = Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=n_dim,\n",
    "    min_count=min_count,\n",
    "    window= window,\n",
    "    sample=downsampling\n",
    ")\n",
    "## build the vocabulary\n",
    "twit_w2v.build_vocab(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train w2v model \n",
    "corpus_count = twit_w2v.corpus_count\n",
    "iteration = 5\n",
    "if gensim.__version__[0] =='1':\n",
    "    twit_w2v.train(x_train)\n",
    "else:\n",
    "    twit_w2v.train(x_train,total_examples=corpus_count,epochs = iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save trained model \n",
    "import os \n",
    "\n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "    twit_w2v.save(os.path.join('trained','twit.w2v'))\n",
    "else:\n",
    "    twit_w2v = Word2Vec.load(os.path.join('trained','twit.w2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wat', 0.6028522253036499),\n",
       " ('...what', 0.5512292981147766),\n",
       " ('how', 0.5442442893981934),\n",
       " (\"what'd\", 0.5403693914413452),\n",
       " ('wot', 0.5325145721435547)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so obviously, we need to tokenize it better \n",
    "twit_w2v.most_similar('what',topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we are going to use word embeding to create a document term matrix. \n",
    "it is kind of a naive way of doing it, simply aggregate all tokens in a sentence and average it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = twit_w2v.wv ## wv is just easier to work with\n",
    "vocabs = model.vocab.keys()\n",
    "del twit_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['got', 'to', 'reconnect', 'with', 'some', 'dear', 'friends', 'tonight.', 'i', 'am', 'so', 'lucky', 'to', 'have', 'so', 'many', 'great', 'people', 'in', 'my', 'life.', 'i', 'am', 'blessed']\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i = 911001\n",
    "print(x_train[i])\n",
    "print(y_train[i])\n",
    "### 0 is negative 1 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildDocumentVector(text,size):\n",
    "    text = [t for t in text if t in vocabs]\n",
    "    if len(text)==0:\n",
    "        return None\n",
    "    else:\n",
    "        vec = [model[t] for t in text]\n",
    "        return np.stack(vec,axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test =\"i am so happy to be here\"\n",
    "buildDocumentVector(test,300).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train vector length:  1594001\n",
      "y_tain classification:  1594001\n"
     ]
    }
   ],
   "source": [
    "## mean standard normalize out input data \n",
    "from sklearn.preprocessing import scale \n",
    "\n",
    "doc_term = [buildDocumentVector(z,n_dim) for z in x_train]\n",
    "\n",
    "for inx, vec in enumerate(doc_term):\n",
    "    if vec is None: y_train[inx]=None\n",
    "\n",
    "train_vecs = [x for x in doc_term if x is not None]\n",
    "train_y = [x for x in y_train if x is not None ]\n",
    "print('x_train vector length: ',len(train_vecs))\n",
    "print('y_tain classification: ',len(train_y))\n",
    "\n",
    "del doc_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split training and testing data \n",
    "from sklearn.cross_validation import train_test_split\n",
    "x_train_vec,x_test_vec,y_train,y_test = train_test_split(train_vecs,train_y,test_size=0.2)\n",
    "x_train_vec = scale(x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.699295171596\n"
     ]
    }
   ],
   "source": [
    "## classification algorism \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log',penalty='l1')\n",
    "lr.fit(x_train_vec,y_train)\n",
    "\n",
    "print('test accuracy:', lr.score(x_test_vec,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try use nero network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_onehot = to_categorical(y_train,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[1])\n",
    "print(y_train_onehot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### build a basic network \n",
    "def build_model(keep_prob):\n",
    "    tf.reset_default_graph()\n",
    "    net = tflearn.input_data([None,n_dim])\n",
    "    net = tflearn.fully_connected(net,300,activation='ReLU')\n",
    "    net = tflearn.fully_connected(net,20,activation='ReLU')\n",
    "    ## output layer \n",
    "    net = tflearn.fully_connected(net,2,activation='softmax')\n",
    "    net = tflearn.regression(net,optimizer='sgd',\n",
    "                            learning_rate=0.005,\n",
    "                            loss='categorical_crossentropy')\n",
    "    model = tflearn.DNN(net)\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 24909  | total loss: \u001b[1m\u001b[32m0.45577\u001b[0m\u001b[0m | time: 10.582s\n",
      "| SGD | epoch: 010 | loss: 0.45577 - acc: 0.7886 -- iter: 1274880/1275200\n",
      "Training Step: 24910  | total loss: \u001b[1m\u001b[32m0.45308\u001b[0m\u001b[0m | time: 10.586s\n",
      "| SGD | epoch: 010 | loss: 0.45308 - acc: 0.7902 -- iter: 1275200/1275200\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "keep_prob=0.5\n",
    "model_net = build_model(keep_prob)\n",
    "model_net.fit(x_train_vec,y_train_onehot,show_metric=True,batch_size=512,n_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test neuro network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.792045821688\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(model_net.predict(scale(x_test_vec))[:,1]>0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == np.array(y_test))\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i love like this place\"\n",
    "sample = x_test_vec.copy()\n",
    "input_data = [buildDocumentVector(text,300)]\n",
    "sample.append(input_data)\n",
    "input_data = scale(sample)\n",
    "#model_net.predict(input_data)[:,1]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get vector from google word to vect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## document to matrix, by looking up wordvectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the entire list of \"words\" from the Google Word2Vec model, and write\n",
    "# these out to text files so we can peruse them.\n",
    "vocab = model.vocab.keys()\n",
    "wordsInVocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(model.vocab)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
