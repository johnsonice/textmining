{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Example in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[http://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('okay', 0.8567795753479004),\n",
       " ('alright', 0.807797372341156),\n",
       " ('OK', 0.6864467859268188),\n",
       " ('lol', 0.6789620518684387),\n",
       " ('anyways', 0.6699042320251465)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('ok',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192315101624),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word to vector\n",
    "model['me'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = '../../twit_data/training.csv'\n",
    "data = pd.read_csv(file_path,encoding='latin1',header=None)\n",
    "columns = ['sentiment','text']\n",
    "data=data[[0,5]]\n",
    "data.columns=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huang/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "## keep only positive and negeative values \n",
    "data = data[((data['sentiment']==4) | (data['sentiment']==0))]\n",
    "data['sentiment'][data['sentiment']==4]  = 1 \n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## clean up text and make them into list of tokens \n",
    "def clean_text(text):\n",
    "    return text.lower().replace('\\n','').split()\n",
    "\n",
    "data['text'] = data['text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## assign training and testing data \n",
    "x_train = data['text']\n",
    "y_train = data['sentiment']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### initialize model and build vocabulary \n",
    "n_dim = 300\n",
    "window = 7 \n",
    "downsampling = 0.0001\n",
    "seed = 1 \n",
    "num_workers = 4\n",
    "min_count = 10 \n",
    "twit_w2v = Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=n_dim,\n",
    "    min_count=min_count,\n",
    "    window= window,\n",
    "    sample=downsampling\n",
    ")\n",
    "## build the vocabulary\n",
    "twit_w2v.build_vocab(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train w2v model \n",
    "corpus_count = twit_w2v.corpus_count\n",
    "iteration = 10\n",
    "if gensim.__version__[0] =='1':\n",
    "    twit_w2v.train(x_train)\n",
    "else:\n",
    "    twit_w2v.train(x_train,total_examples=corpus_count,epochs = iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save trained model \n",
    "import os \n",
    "\n",
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "    twit_w2v.save(os.path.join('trained','twit.w2v'))\n",
    "else:\n",
    "    twit_w2v = Word2Vec.load(os.path.join('trained','twit.w2v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seeing', 0.6153379678726196),\n",
       " ('concert..', 0.5914677381515503),\n",
       " ('c', 0.569611668586731),\n",
       " ('watch:', 0.5642238259315491),\n",
       " ('together!!!', 0.5602788925170898)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so obviously, we need to tokenize it better \n",
    "twit_w2v.most_similar('see',topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we are going to use word embeding to create a document term matrix. \n",
    "it is kind of a naive way of doing it, simply aggregate all tokens in a sentence and average it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = twit_w2v.wv ## wv is just easier to work with\n",
    "vocabs = model.vocab.keys()\n",
    "del twit_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildDocumentVector(text,size):\n",
    "    text = [t for t in text if t in vocabs]\n",
    "    if len(text)==0:\n",
    "        return None\n",
    "    else:\n",
    "        vec = [model[t] for t in text]\n",
    "        return np.stack(vec,axis=0).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "x_train vector length:  1594001\n",
      "y_tain classification:  1594001\n"
     ]
    }
   ],
   "source": [
    "## mean standard normalize out input data \n",
    "from sklearn.preprocessing import scale \n",
    "\n",
    "doc_term = [buildDocumentVector(z,n_dim) for z in x_train]\n",
    "\n",
    "for inx, vec in enumerate(doc_term):\n",
    "    if vec is None: y_train[inx]=None\n",
    "\n",
    "train_vecs = [x for x in doc_term if x is not None]\n",
    "train_y = [x for x in y_train if x is not None ]\n",
    "print('x_train vector length: ',len(train_vecs))\n",
    "print('y_tain classification: ',len(train_y))\n",
    "\n",
    "del doc_term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## split training and testing data \n",
    "from sklearn.cross_validation import train_test_split\n",
    "x_train_vec,x_test_vec,y_train,y_test = train_test_split(train_vecs,train_y,test_size=0.2)\n",
    "x_train_vec = scale(x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275200   1275200\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_vec),\" \",len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.642877531752\n"
     ]
    }
   ],
   "source": [
    "## classification algorism \n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "lr = SGDClassifier(loss='log',penalty='l1')\n",
    "lr.fit(x_train_vec,y_train)\n",
    "\n",
    "print('training accuracy:', lr.score(x_test_vec,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try use nero network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_onehot = to_categorical(y_train,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### build a basic network \n",
    "def build_model():\n",
    "    tf.reset_default_graph()\n",
    "    net = tflearn.input_data([None,n_dim])\n",
    "    net = tflearn.fully_connected(net,200,activation='ReLU')\n",
    "    net = tflearn.fully_connected(net,50,activation='ReLU')\n",
    "    ## output layer \n",
    "    net = tflearn.fully_connected(net,2,activation='softmax')\n",
    "    net = tflearn.regression(net,optimizer='sgd',\n",
    "                            learning_rate=0.01,\n",
    "                            loss='categorical_crossentropy')\n",
    "    model = tflearn.DNN(net)\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 132928  | total loss: \u001b[1m\u001b[32m0.38996\u001b[0m\u001b[0m | time: 18.183s\n",
      "\u001b[2K\r",
      "| SGD | epoch: 027 | loss: 0.38996 - acc: 0.8230 -- iter: 0869376/1275200\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(x_train_vec,y_train_onehot,show_metric=True,batch_size=256,n_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test neuro network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = np.array(model.predict(x_test_vec)[:,0]>0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == y_test)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get vector from google word to vect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## document to matrix, by looking up wordvectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the entire list of \"words\" from the Google Word2Vec model, and write\n",
    "# these out to text files so we can peruse them.\n",
    "vocab = model.vocab.keys()\n",
    "wordsInVocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.vocab)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
