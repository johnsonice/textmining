{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised, sentiment classifer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download_shell()\n",
    "# install punkt\n",
    "# install wordnet \n",
    "# install stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## setup setming and stop words \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "## you can also import your own stopword list from a file \n",
    "## from http://www.lextek.com/manuals/onix/stopwords1.html\n",
    "#  stopwords = set(w.rstrip() for w in open('stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the reviews\n",
    "# data courtesy of http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html\n",
    "positive_reviews = BeautifulSoup(open('data/positive.review').read(),'lxml')\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('data/negative.review').read(),'lxml')\n",
    "negative_reviews = negative_reviews.findAll('review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there are more positive reviews than negative reviews\n",
    "# so let's take a random sample so we have balanced classes\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our own tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we built the tokenizer and coutvectornizer by ourselves. \n",
    "- You can also just use (from sklearn.feature_extraction.text import CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    s = s.lower() # downcase\n",
    "    tokens = nltk.tokenize.word_tokenize(s) # split string into words (tokens)\n",
    "    tokens = [t for t in tokens if len(t) > 2] # remove short words, they're probably not useful\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n",
    "    tokens = [t for t in tokens if t not in stopwords] # remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# now let's create our input matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x = np.zeros(len(word_index_map) + 1) # last element is for the label\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        x[i] += 1\n",
    "    x = x / x.sum() # normalize it before setting label\n",
    "    x[-1] = label\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'love', 'dog', 'hate', 'dog']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see how tokenizer works \n",
    "s = 'i am a dog, i love dog, i hate dog.'\n",
    "test = my_tokenizer(s)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a word-to-index map so that we can create our word-frequency vectors later\n",
    "# let's also save the tokenized versions so we don't have to tokenize again later\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)              ## list of list of tokens from each review\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:            ## check if string is in dict keys \n",
    "            word_index_map[token] = current_index  ## if it is a new word, add it in \n",
    "            current_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1,  0.1,  0.1, ...,  0. ,  0. ,  1. ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see how token to vector works \n",
    "tokens_to_vector(positive_tokenized[0],1)   ## it return an np array with weights for each \n",
    "                                            ## token and the label in the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## N is total number of our sample \n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "# create data matrix, it is N by D+1 matrix, the last column is the label \n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "\n",
    "## populate the data matrix, first D columns are token weights and the last column\n",
    "## is the label, which is y \n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "\n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, you can also use sklearn's train test split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.73\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data and create train/test splits\n",
    "# try it multiple times!\n",
    "np.random.shuffle(data)\n",
    "\n",
    "X = data[:,:-1]\n",
    "Y = data[:,-1]\n",
    "\n",
    "# last 100 rows will be test\n",
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print (\"Classification rate:\", model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doe -1.00399035607\n",
      "n't -1.8337964508\n",
      "got -0.522858283334\n",
      "used 0.970138434043\n",
      "wa -1.22844061761\n",
      "thing -0.886798265534\n",
      "first -0.696443610195\n",
      "use 1.5977817017\n",
      "would -0.746891304266\n",
      "time -0.691433904607\n",
      "small 0.620001363941\n",
      "good 1.84791058138\n",
      "even -0.821505657775\n",
      "ha 0.601765583697\n",
      "month -0.644060450518\n",
      "love 0.9434789592\n",
      "price 2.21187285848\n",
      "could -0.533452355634\n",
      "great 3.51559489587\n",
      "get -1.13178842648\n",
      "unit -0.655115825176\n",
      "buy -0.883206933315\n",
      "highly 0.851376414431\n",
      "easy 1.36289010044\n",
      "back -1.49552251254\n",
      "little 0.768537093789\n",
      "well 0.962787909159\n",
      "sound 0.94960200689\n",
      "perfect 0.857698782345\n",
      "recommend 0.582425260976\n",
      "returned -0.685497076112\n",
      "need 0.591166382566\n",
      "support -0.795699717634\n",
      "quality 1.28582714231\n",
      "excellent 1.23723995839\n",
      "working -0.519232188527\n",
      "worked -0.780555669843\n",
      "'ve 0.530529403781\n",
      "week -0.540690995688\n",
      "cable 0.610543117268\n",
      "like 0.562175300597\n",
      "lot 0.533482679923\n",
      "speaker 0.777110105281\n",
      "item -0.948495084741\n",
      "money -0.947202996709\n",
      "best 0.96136333176\n",
      "try -0.541867648287\n",
      "pretty 0.590752311103\n",
      "two -0.659403035043\n",
      "problem 0.554023260388\n",
      "comfortable 0.549010689348\n",
      "memory 0.815215593928\n",
      "using 0.55292045323\n",
      "customer -0.56752015873\n",
      "bad -0.538720250547\n",
      "fast 0.739971051704\n",
      "return -0.988885906552\n",
      "warranty -0.503280767171\n",
      "tried -0.682166853197\n",
      "poor -0.682263340011\n",
      "waste -0.880814240407\n"
     ]
    }
   ],
   "source": [
    "# let's look at the weights for each word\n",
    "# try it with different threshold values!\n",
    "\n",
    "## basically print out the coefficient \n",
    "threshold = 0.5\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print (word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
